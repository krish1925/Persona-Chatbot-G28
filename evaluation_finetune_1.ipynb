{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# definitions for carrying out bleu score calculations for two outputs\n",
    "import nltk\n",
    "import subprocess\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "def bleu_score(hypothesis, reference):\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    return sentence_bleu([reference], hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L F-measure: 1.0\n",
      "ROUGE-L F-measure: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# !pip install rouge-score\n",
    "import rouge_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge_score(hypothesis, reference):\n",
    "   \n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "\n",
    "\n",
    "hypothesis = \"The quick brown fox jumps over the lazy dog\"\n",
    "reference = \"The quick brown fox jumped over the lazy dog\"\n",
    "rouge_l_fmeasure = calculate_rouge_score(hypothesis, reference)\n",
    "print(f\"ROUGE-L F-measure: {rouge_l_fmeasure}\")\n",
    "\n",
    "hypothesis = \"The slow purple fox jumps over the lazy fox\"\n",
    "\n",
    "rouge_Scores = calculate_rouge_score(hypothesis, reference)\n",
    "print(f\"ROUGE-L F-measure: {rouge_Scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/krishpatel/anaconda3/lib/python3.11/site-packages/litellm-1.40.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /Users/krishpatel/anaconda3/lib/python3.11/site-packages/openai-0.27.7-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert_score in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from bert_score) (2.3.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from bert_score) (2.0.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from bert_score) (4.32.1)\n",
      "Requirement already satisfied: numpy in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from bert_score) (1.23.5)\n",
      "Requirement already satisfied: requests in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from bert_score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from bert_score) (4.62.1)\n",
      "Requirement already satisfied: matplotlib in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from bert_score) (3.7.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from bert_score) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2023.3)\n",
      "Requirement already satisfied: filelock in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (2022.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.17.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from matplotlib->bert_score) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from matplotlib->bert_score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from matplotlib->bert_score) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from matplotlib->bert_score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from matplotlib->bert_score) (10.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from matplotlib->bert_score) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from requests->bert_score) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from requests->bert_score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from requests->bert_score) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from requests->bert_score) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.5 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/krishpatel/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishpatel/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/krishpatel/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# !pip install bert_score\n",
    "subprocess.run([\"pip\", \"install\", \"bert_score\"]) \n",
    "import bert_score\n",
    "\n",
    "def calculate_bertscore(hypotheses, references, model_type='bert-base-uncased'):\n",
    "    P, R, F1 = bert_score.score(hypotheses, references, model_type=model_type, lang=\"en\")\n",
    "\n",
    "    return {\n",
    "        'precision': P.mean().item(),\n",
    "        'recall': R.mean().item(),\n",
    "        'f1': F1.mean().item()\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "hypotheses = [\"The quick brown fox jumps over the lazy dog\"]\n",
    "references = [\"The quick brown fox jumped over the lazy dog\"]\n",
    "bertscore_result = calculate_bertscore(hypotheses, references)\n",
    "print(f\"BERTScore: {bertscore_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5.426182746887207\n"
     ]
    }
   ],
   "source": [
    "# ADD PERPLEXITY\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def calculate_perplexity(text, model_name='gpt2'):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        loss = model(input_ids, labels=input_ids)[0]\n",
    "    return loss.item()\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "perplexity = calculate_perplexity(text)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# !pip install python-dotenv\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      7\u001b[0m load_dotenv()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "# !pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = api_key\n",
    "client = OpenAI(api_key=\"sk-proj-4Brh6XVArOOG4c7DMUI7T3BlbkFJIZrEaupeqYNS23AIYpnz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline example\n",
    "\n",
    "def generate_persona_prompt(prompt, context):\n",
    "    full_prompt = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are a chatbot with the goal of responding to my prompt. \\\n",
    "     I will be user 1, and you will be user 2. I will provide the personas only for user 2 \\\n",
    "     I will also provide the first few parts of the conversation between the 2 users with the \\\n",
    "     given persona, from which you will have to infer the person of user1 and respond accordingly. Given the persona of user 2 (your persona) and the beginning of the conversation, \\\n",
    "     you will need to reply to my prompt as if you were that user, and take on that user's personality \\\n",
    "     based on the description provided. Here are the personas of user 2, \\\n",
    "     and beginning of their conversation: {context}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "  ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",  \n",
    "        messages=full_prompt\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage:\n",
    "# persona = \"A fourth-year computer science major at UCLA interested in machine learning, currently on the lookout for internships, and enjoys learning.\"\n",
    "# text = \"Tell me about your internship!\"\n",
    "\n",
    "# response = generate_persona_prompt(text, persona)\n",
    "# print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuned example\n",
    "\n",
    "def generate_persona_prompt_finetuned(prompt, context):\n",
    "    full_prompt = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are a chatbot with the goal of responding to my prompt. \\\n",
    "     I will be user 1, and you will be user 2. I will provide the personas only for user 2 \\\n",
    "     I will also provide the first few parts of the conversation between the 2 users with the \\\n",
    "     given persona, from which you will have to infer the person of user1 and respond accordingly. Given the persona of user 2 (your persona) and the beginning of the conversation, \\\n",
    "     you will need to reply to my prompt as if you were that user, and take on that user's personality \\\n",
    "     based on the description provided. Here are the personas of user 2, \\\n",
    "     and beginning of their conversation: {context}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "  ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"ft:gpt-3.5-turbo-0125:personal::9WTbkMz7\",  \n",
    "        messages=full_prompt\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage:\n",
    "# persona = \"A fourth-year computer science major at UCLA interested in machine learning, currently on the lookout for internships, and enjoys learning.\"\n",
    "# text = \"Tell me about your internship!\"\n",
    "\n",
    "# response = generate_persona_prompt_finetuned(text, persona)\n",
    "# print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Synthetic-Persona-Chat_valid.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extracted_data\n\u001b[1;32m     34\u001b[0m csv_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSynthetic-Persona-Chat_valid.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 35\u001b[0m extracted_data \u001b[38;5;241m=\u001b[39m extract_conversation_parts(csv_filename)\n\u001b[1;32m     37\u001b[0m trunc_extracted_data \u001b[38;5;241m=\u001b[39m extracted_data[:\u001b[38;5;241m15\u001b[39m]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m trunc_extracted_data:\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mextract_conversation_parts\u001b[0;34m(csv_filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_conversation_parts\u001b[39m(csv_filename):\n\u001b[0;32m----> 4\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_filename)\n\u001b[1;32m      6\u001b[0m     extracted_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Synthetic-Persona-Chat_valid.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_conversation_parts(csv_filename):\n",
    "    data = pd.read_csv(csv_filename)\n",
    "    \n",
    "    extracted_data = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        user1_persona = row['user 1 personas']\n",
    "        user2_persona = row['user 2 personas']\n",
    "        conversation = row['Best Generated Conversation']\n",
    "        \n",
    "        conversation_lines = conversation.split('\\n')\n",
    "        \n",
    "        if len(conversation_lines) >= 7:\n",
    "            context = \"\\n\".join(conversation_lines[:6])\n",
    "            prompt = conversation_lines[6]\n",
    "            full_continued_convo = \"\\n\".join(conversation_lines[7:])\n",
    "            single_continued_convo = conversation_lines[7]\n",
    "            \n",
    "            extracted_data.append({\n",
    "                'user1_persona': user1_persona,\n",
    "                'user2_persona': user2_persona,\n",
    "                'context': context,\n",
    "                'prompt': prompt,\n",
    "                'finished_convo': full_continued_convo,\n",
    "                'prompt_response': single_continued_convo\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Conversation is too short in row {row.name}\")\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "csv_filename = r'Synthetic-Persona-Chat_valid.csv'\n",
    "extracted_data = extract_conversation_parts(csv_filename)\n",
    "\n",
    "trunc_extracted_data = extracted_data[:15]\n",
    "\n",
    "for data in trunc_extracted_data:\n",
    "    print(\"User 1 Persona:\", data['user1_persona'])\n",
    "    print(\"User 2 Persona:\", data['user2_persona'])\n",
    "    print(\"Context:\", data['context'])\n",
    "    print(\"Prompt:\", data['prompt'])\n",
    "    print(\"Expected response:\", data['prompt_response'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: User 2: Besides moving objects, I enjoy meditating to calm my mind.\n",
      "User 2: I also like watching documentaries about science and space.\n",
      "User 1: That's really interesting! Do you have a favorite documentary?\n",
      "User 2: Yes, I love anything that explores the mysteries of the universe.\n",
      "User 1: Sounds like you have a curious mind! What else fascinates you about space?\n",
      "\n",
      "Generated Response: User 2: Yeah, it's opened my eyes to a lot of things. I never gave much thought to animal rights before.\n",
      "\n",
      "Generated Response: User 2: Yeah, it's a classic. Have you seen any other good war documentaries recently?\n",
      "\n",
      "Generated Response: User 2: Investment banking is lucrative but intense. Have you thought about other career paths in finance? It's always good to explore your options before committing. What draws you to investment banking specifically?\n",
      "\n",
      "Generated Response: User 2: Definitely, live music is always better than listening to recordings. The energy is incredible at concerts. Plus, it's a great way to unwind after a long week at work. I can't wait to get there and just enjoy the show without any distractions.\n",
      "\n",
      "Generated Response: User 2: It's amazing how dogs can become such an important part of our lives, isn't it? They're so loyal and loving. Barnaby must bring a lot of joy into your life.\n",
      "\n",
      "Generated Response: User 2: I'm Sarah. Nice to meet you! By the way, do you have any favorite skateboarding tricks?\n",
      "\n",
      "Generated Response: User 2: Riding horses is so therapeutic for me. It's this amazing connection with nature that I can't get enough of. Do you have a favorite place to ride?\n",
      "\n",
      "Generated Response: User 2: That sounds exhilarating! I admire your bravery. Have you ever had any close calls or scary moments while performing stunts?\n",
      "\n",
      "Generated Response: User 2: New Kids on the Block! I remember them from the 80s. They had some catchy tunes. Do you have a favorite song of theirs?\n",
      "\n",
      "Generated Response: User 2: Roses are beautiful! I prefer growing sunflowers in my garden; they are so vibrant and uplifting. Do you have any secret tips for keeping your roses healthy and blooming?\n",
      "\n",
      "Generated Response: User 2: You definitely should! The pasta there is just incredibly delicious. Have you ever tried making your own pasta at home?\n",
      "\n",
      "Generated Response: User 2: Running is such a fantastic way to stay active and clear the mind. Do you have a favorite route you enjoy running?\n",
      "\n",
      "Generated Response: User 2: \"Toxic\" is my absolute favorite! It always gets me in the mood for a good tanning session. What do you like to do with your family?\n",
      "\n",
      "Generated Response: User 2: Sounds interesting. Do you enjoy the cold weather in Canada?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline validation\n",
    "def process_validation_data(csv_filename):\n",
    "    extracted_data = extract_conversation_parts(csv_filename)\n",
    "    trunc_extracted_data = extracted_data[:15]  # Taking only the first 15 rows\n",
    "    \n",
    "    model_responses = []\n",
    "    \n",
    "    for data in trunc_extracted_data:\n",
    "        context = f\"User 1: {data['user1_persona']}\\n\\n User 2: {data['user2_persona']}\\n \\\n",
    "            First 6 pieces of conversation: {data['context']}\"\n",
    "        prompt = f\"{data['prompt']}\"\n",
    "        generated_response = generate_persona_prompt(prompt, context)\n",
    "        model_responses.append(generated_response)\n",
    "    \n",
    "    return model_responses\n",
    "\n",
    "csv_filename = r'Synthetic-Persona-Chat_valid.csv'\n",
    "\n",
    "model_responses = process_validation_data(csv_filename)\n",
    "\n",
    "for response in model_responses:\n",
    "    print(\"Generated Response:\", response)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: User 2: I also had to have a transplant when I was younger.\n",
      "\n",
      "Generated Response: User 2: Me too! I love animals so much. I wish I could have a pet, but I can't have one in here.\n",
      "\n",
      "Generated Response: User 2: Yes, it's very interesting.\n",
      "\n",
      "Generated Response: User 2: That's a good field to go into. It can be very lucrative.\n",
      "\n",
      "Generated Response: User 2: Me too. I've heard they're even better in person.\n",
      "\n",
      "Generated Response: User 2: I bet he is. Dogs are the best companions.\n",
      "\n",
      "Generated Response: User 2: My name is Sarah.\n",
      "\n",
      "Generated Response: User 2: Do you have a favorite kind of horse to ride?\n",
      "\n",
      "Generated Response: User 2: That sounds really exciting! Do you ever get scared?\n",
      "\n",
      "Generated Response: User 2: Oh, I love New Kids on the Block! My favorite song is \"Step By Step.\"\n",
      "\n",
      "Generated Response: User 2: Roses are beautiful! I also like to grow sunflowers and daisies.\n",
      "\n",
      "Generated Response: User 2: You definitely should. I think you'll really like it.\n",
      "\n",
      "Generated Response: User 2: It's a great way to stay active and clear your mind.\n",
      "\n",
      "Generated Response: User 2: My favorite Britney Spears song is Toxic.\n",
      "\n",
      "Generated Response: User 2: Oh, I've always wanted to visit Canada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_validation_data_finetuned(csv_filename):\n",
    "    extracted_data = extract_conversation_parts(csv_filename)\n",
    "    trunc_extracted_data = extracted_data[:15]  # Taking only the first 15 rows\n",
    "    \n",
    "    model_responses = []\n",
    "    \n",
    "    for data in trunc_extracted_data:\n",
    "        context = f\"User 1: {data['user1_persona']}\\n\\n User 2: {data['user2_persona']}\\n \\\n",
    "            First 6 pieces of conversation: {data['context']}\"\n",
    "        prompt = f\"{data['prompt']}\"\n",
    "        generated_response = generate_persona_prompt_finetuned(prompt, context)\n",
    "        model_responses.append(generated_response)\n",
    "    \n",
    "    return model_responses\n",
    "\n",
    "csv_filename = r'Synthetic-Persona-Chat_valid.csv'\n",
    "\n",
    "model_responses_finetuned = process_validation_data_finetuned(csv_filename)\n",
    "\n",
    "for response in model_responses_finetuned:\n",
    "    print(\"Generated Response:\", response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User 2: Besides moving objects, I enjoy meditating to calm my mind.',\n",
       " \"User 2: Yeah, it's opened my eyes to a lot of things. I never gave much thought to animal rights before.\",\n",
       " \"User 2: Yeah, it's a classic. Have you seen any other good war documentaries recently?\",\n",
       " \"User 2: Investment banking is lucrative but intense. Have you thought about other career paths in finance? It's always good to explore your options before committing. What draws you to investment banking specifically?\",\n",
       " \"User 2: Definitely, live music is always better than listening to recordings. The energy is incredible at concerts. Plus, it's a great way to unwind after a long week at work. I can't wait to get there and just enjoy the show without any distractions.\",\n",
       " \"User 2: It's amazing how dogs can become such an important part of our lives, isn't it? They're so loyal and loving. Barnaby must bring a lot of joy into your life.\",\n",
       " \"User 2: I'm Sarah. Nice to meet you! By the way, do you have any favorite skateboarding tricks?\",\n",
       " \"User 2: Riding horses is so therapeutic for me. It's this amazing connection with nature that I can't get enough of. Do you have a favorite place to ride?\",\n",
       " 'User 2: That sounds exhilarating! I admire your bravery. Have you ever had any close calls or scary moments while performing stunts?',\n",
       " 'User 2: New Kids on the Block! I remember them from the 80s. They had some catchy tunes. Do you have a favorite song of theirs?',\n",
       " 'User 2: Roses are beautiful! I prefer growing sunflowers in my garden; they are so vibrant and uplifting. Do you have any secret tips for keeping your roses healthy and blooming?',\n",
       " 'User 2: You definitely should! The pasta there is just incredibly delicious. Have you ever tried making your own pasta at home?',\n",
       " 'User 2: Running is such a fantastic way to stay active and clear the mind. Do you have a favorite route you enjoy running?',\n",
       " 'User 2: \"Toxic\" is my absolute favorite! It always gets me in the mood for a good tanning session. What do you like to do with your family?',\n",
       " 'User 2: Sounds interesting. Do you enjoy the cold weather in Canada?']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_expected_responses = [data['prompt_response'] for data in trunc_extracted_data]\n",
    "full_expected_responses = [data['finished_convo'] for data in trunc_extracted_data]\n",
    "\n",
    "single_model_responses = [response.split('\\n')[0] for response in model_responses]\n",
    "single_model_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User 2: I also had to have a transplant when I was younger.',\n",
       " \"User 2: Me too! I love animals so much. I wish I could have a pet, but I can't have one in here.\",\n",
       " \"User 2: Yes, it's very interesting.\",\n",
       " \"User 2: That's a good field to go into. It can be very lucrative.\",\n",
       " \"User 2: Me too. I've heard they're even better in person.\",\n",
       " 'User 2: I bet he is. Dogs are the best companions.',\n",
       " 'User 2: My name is Sarah.',\n",
       " 'User 2: Do you have a favorite kind of horse to ride?',\n",
       " 'User 2: That sounds really exciting! Do you ever get scared?',\n",
       " 'User 2: Oh, I love New Kids on the Block! My favorite song is \"Step By Step.\"',\n",
       " 'User 2: Roses are beautiful! I also like to grow sunflowers and daisies.',\n",
       " \"User 2: You definitely should. I think you'll really like it.\",\n",
       " \"User 2: It's a great way to stay active and clear your mind.\",\n",
       " 'User 2: My favorite Britney Spears song is Toxic.',\n",
       " \"User 2: Oh, I've always wanted to visit Canada.\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_model_responses_finetuned = [response.split('\\n')[0] for response in model_responses_finetuned]\n",
    "single_model_responses_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: {'precision': 0.6617485880851746, 'recall': 0.5241777300834656, 'f1': 0.5815606713294983}\n"
     ]
    }
   ],
   "source": [
    "bertscore_result = calculate_bertscore(single_expected_responses, single_model_responses)\n",
    "print(f\"BERTScore: {bertscore_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: {'precision': 0.6779415607452393, 'recall': 0.6920197010040283, 'f1': 0.6822169423103333}\n"
     ]
    }
   ],
   "source": [
    "bertscore_result_finetuned = calculate_bertscore(single_expected_responses, single_model_responses_finetuned)\n",
    "print(f\"BERTScore: {bertscore_result_finetuned}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
