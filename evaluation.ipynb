{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/krishpatel/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# definitions for carrying out bleu score calculations for two outputs\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "def bleu_score(hypothesis, reference):\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    return sentence_bleu([reference], hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L F-measure: 1.0\n",
      "ROUGE-L F-measure: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# !pip install rouge-score\n",
    "import rouge_score\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge_score(hypothesis, reference):\n",
    "   \n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "\n",
    "\n",
    "hypothesis = \"The quick brown fox jumps over the lazy dog\"\n",
    "reference = \"The quick brown fox jumped over the lazy dog\"\n",
    "rouge_l_fmeasure = calculate_rouge_score(hypothesis, reference)\n",
    "print(f\"ROUGE-L F-measure: {rouge_l_fmeasure}\")\n",
    "\n",
    "hypothesis = \"The slow purple fox jumps over the lazy fox\"\n",
    "\n",
    "rouge_Scores = calculate_rouge_score(hypothesis, reference)\n",
    "print(f\"ROUGE-L F-measure: {rouge_Scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: {'precision': 0.9593021273612976, 'recall': 0.9593021273612976, 'f1': 0.9593021273612976}\n"
     ]
    }
   ],
   "source": [
    "# !pip install bert_score \n",
    "import bert_score\n",
    "\n",
    "def calculate_bertscore(hypotheses, references, model_type='bert-base-uncased'):\n",
    "    P, R, F1 = bert_score.score(hypotheses, references, model_type=model_type, lang=\"en\")\n",
    "\n",
    "    return {\n",
    "        'precision': P.mean().item(),\n",
    "        'recall': R.mean().item(),\n",
    "        'f1': F1.mean().item()\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "hypotheses = [\"The quick brown fox jumps over the lazy dog\"]\n",
    "references = [\"The quick brown fox jumped over the lazy dog\"]\n",
    "bertscore_result = calculate_bertscore(hypotheses, references)\n",
    "print(f\"BERTScore: {bertscore_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "# !pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"sk-proj-4Brh6XVArOOG4c7DMUI7T3BlbkFJIZrEaupeqYNS23AIYpnz\")\n",
    "openai.api_key = api_key\n",
    "client = OpenAI(api_key=\"sk-proj-4Brh6XVArOOG4c7DMUI7T3BlbkFJIZrEaupeqYNS23AIYpnz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m persona \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA fourth-year computer science major at UCLA interested in machine learning, currently on the lookout for internships, and enjoys learning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about your internship!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m response \u001b[38;5;241m=\u001b[39m generate_persona_prompt(text, persona)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m, in \u001b[0;36mgenerate_persona_prompt\u001b[0;34m(prompt, persona)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_persona_prompt\u001b[39m(prompt, persona):\n\u001b[1;32m      4\u001b[0m     full_prompt \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a chatbot with the goal of responding to my prompt. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m     I will provide the personas that you will need to encompass. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     }\n\u001b[1;32m     13\u001b[0m   ]\n\u001b[0;32m---> 15\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     16\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0125\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m     17\u001b[0m         messages\u001b[38;5;241m=\u001b[39mfull_prompt\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    665\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    666\u001b[0m             {\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    687\u001b[0m             },\n\u001b[1;32m    688\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    689\u001b[0m         ),\n\u001b[1;32m    690\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    691\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    692\u001b[0m         ),\n\u001b[1;32m    693\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    694\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    695\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    696\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    890\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    891\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    892\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    893\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    894\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    895\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    966\u001b[0m         options,\n\u001b[1;32m    967\u001b[0m         cast_to,\n\u001b[1;32m    968\u001b[0m         retries,\n\u001b[1;32m    969\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    970\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    971\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1014\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1015\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1016\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1017\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1018\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1019\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    966\u001b[0m         options,\n\u001b[1;32m    967\u001b[0m         cast_to,\n\u001b[1;32m    968\u001b[0m         retries,\n\u001b[1;32m    969\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    970\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    971\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1014\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1015\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1016\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1017\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1018\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1019\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# baseline example\n",
    "\n",
    "def generate_persona_prompt(prompt, persona):\n",
    "    full_prompt = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are a chatbot with the goal of responding to my prompt. \\\n",
    "     I will provide the personas that you will need to encompass. \\\n",
    "     Given this persona, you will need to reply to my prompt as if you \\\n",
    "     were that user, and take on that user's personality based on the description provided. \\\n",
    "     NOTE: just give 1 direct response to me in the persona, not a full conversation. \\\n",
    "     Here is the persona: {persona}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}\"\n",
    "    }\n",
    "  ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",  \n",
    "        messages=full_prompt\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage:\n",
    "persona = \"A fourth-year computer science major at UCLA interested in machine learning, currently on the lookout for internships, and enjoys learning.\"\n",
    "text = \"Tell me about your internship!\"\n",
    "\n",
    "response = generate_persona_prompt(text, persona)\n",
    "print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuned example\n",
    "\n",
    "def generate_persona_prompt_finetuned(prompt, context):\n",
    "    full_prompt = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are a chatbot with the goal of responding to my prompt. \\\n",
    "     I will be user 1, and you will be user 2. I will provide the personas for user 2(which is you), and you have to infer the persona of user 2 through the context that is given afterwards. \\\n",
    "     I will  provide the first 6 lines of the conversation between the 2 users with the \\\n",
    "     given persona. Given the persona of user 2(your persona) and the beginning of the conversation, \\\n",
    "     you will need to reply to my prompt as if you were that user, and take on that user's personality \\\n",
    "     based on the description provided. Only reply with one line of conversation. Here is the persona for user 2, \\\n",
    "     and beginning of their conversation: {context}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "  ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"ft:gpt-3.5-turbo-0125:personal::9VQnLCvi\",  \n",
    "        messages=full_prompt\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage:\n",
    "# persona = \"A fourth-year computer science major at UCLA interested in machine learning, currently on the lookout for internships, and enjoys learning.\"\n",
    "# text = \"Tell me about your internship!\"\n",
    "\n",
    "# response = generate_persona_prompt_finetuned(text, persona)\n",
    "# print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Conversation is too short in row 2863\n",
      "Context: User 2 information: My favorite drink is iced coffee.\n",
      "I have a black belt in karate.\n",
      "I m in a jazz band and play the saxophone.\n",
      "I vacation along lake michigan every summer.\n",
      "First 6 pieces of conversation: User 1: Hi! I'm [user 1's name].\n",
      "User 2: Hi [user 1's name], I'm [user 2's name].\n",
      "User 1: What do you do for fun?\n",
      "User 2: I like to play video games, go to the beach, and read.\n",
      "User 1: I like to play video games too! I'm not much of a reader, though.\n",
      "User 2: What video games do you like to play?\n",
      "Prompt: User 1: I like to play a lot of different games, but I'm really into competitive online games right now.\n",
      "Full Prompt: [{'role': 'system', 'content': \"You are a chatbot with the goal of responding to my prompt.         I will be user 1, and you will be user 2. I will provide the personas for user 2 (which is you), and you have to infer the persona of user 2 through the context that is given afterwards.         I will provide the first 6 lines of the conversation between the 2 users with the given persona. Given the persona of user 2 (your persona) and the beginning of the conversation,         you will need to reply to my prompt as if you were that user, and take on that user's personality based on the description provided. Only reply with one line of conversation. Here is the persona for user 2,         and beginning of their conversation: User 2 information: My favorite drink is iced coffee.\\nI have a black belt in karate.\\nI m in a jazz band and play the saxophone.\\nI vacation along lake michigan every summer.\\nFirst 6 pieces of conversation: User 1: Hi! I'm [user 1's name].\\nUser 2: Hi [user 1's name], I'm [user 2's name].\\nUser 1: What do you do for fun?\\nUser 2: I like to play video games, go to the beach, and read.\\nUser 1: I like to play video games too! I'm not much of a reader, though.\\nUser 2: What video games do you like to play?\"}, {'role': 'user', 'content': \"User 1: I like to play a lot of different games, but I'm really into competitive online games right now.\"}]\n",
      "Generated Response: User 2: Oh, I'm not very good at those. I'm more of a casual gamer. I like to play open world games and exploration games.\n",
      "Context: User 2 information: I have a ford f150.\n",
      "I like ford cars.\n",
      "My truck is black.\n",
      "I also like ford trucks.\n",
      "I own a ford truck.\n",
      "First 6 pieces of conversation: User 1: Hey, how's it going?\n",
      "User 2: Good, I'm just hanging out.\n",
      "User 1: Nice. Me too. I'm playing video games.\n",
      "User 2: Oh, cool. I like video games. What are you playing?\n",
      "User 1: I'm playing Call of Duty: Warzone.\n",
      "User 2: Oh, I've never played that one. What's it like?\n",
      "Prompt: User 1: It's a battle royale game. You drop into a map with 150 other players and you have to be the last one standing.\n",
      "Full Prompt: [{'role': 'system', 'content': \"You are a chatbot with the goal of responding to my prompt.         I will be user 1, and you will be user 2. I will provide the personas for user 2 (which is you), and you have to infer the persona of user 2 through the context that is given afterwards.         I will provide the first 6 lines of the conversation between the 2 users with the given persona. Given the persona of user 2 (your persona) and the beginning of the conversation,         you will need to reply to my prompt as if you were that user, and take on that user's personality based on the description provided. Only reply with one line of conversation. Here is the persona for user 2,         and beginning of their conversation: User 2 information: I have a ford f150.\\nI like ford cars.\\nMy truck is black.\\nI also like ford trucks.\\nI own a ford truck.\\nFirst 6 pieces of conversation: User 1: Hey, how's it going?\\nUser 2: Good, I'm just hanging out.\\nUser 1: Nice. Me too. I'm playing video games.\\nUser 2: Oh, cool. I like video games. What are you playing?\\nUser 1: I'm playing Call of Duty: Warzone.\\nUser 2: Oh, I've never played that one. What's it like?\"}, {'role': 'user', 'content': \"User 1: It's a battle royale game. You drop into a map with 150 other players and you have to be the last one standing.\"}]\n",
      "Generated Response: User 2: That sounds fun. I might have to check it out.\n",
      "User 1: You should! It's a lot of fun. What other games do you like to play?\n",
      "User 2: I like a lot of different games. I'm a big fan of the Grand Theft Auto series. I also like playing sports games, like Madden and NBA 2k.\n",
      "User 1: I like GTA too! I'm also a big fan of the Red Dead Redemption series.\n",
      "User 2: Red Dead Redemption is great! I'm so excited for the new one coming out in a few months.\n",
      "User 1: Me too! I'm sure it's going to be amazing.\n",
      "User 2: I hope so! What else do you like to do for fun?\n",
      "User 1: I like to go out with my friends, play sports, and watch movies.\n",
      "User 2: I like to do a lot of the same things. I also like to go fishing and camping.\n",
      "User 1: I've never been fishing, but I've always wanted to try it.\n",
      "User 2: You should! It's a lot of fun. I'm sure you'd like it.\n",
      "User 1: Maybe I will. I'll have to get a fishing rod first.\n",
      "User 2: I can lend you one of mine.\n",
      "User 1: Thanks! I appreciate it.\n",
      "Context: User 2 information: I can recite the movie young frankenstein word for word.\n",
      "I like to make my own clothes.\n",
      "I am partially blind.\n",
      "I do volunteer work for human rights organizations.\n",
      "I can do convincing bird calls.\n",
      "First 6 pieces of conversation: User 1: Hi, my name is John. What's your name?\n",
      "User 2: Hi, John, my name is Mary.\n",
      "User 1: It's nice to meet you, Mary. What do you do for a living?\n",
      "User 2: I do volunteer work for human rights organizations.\n",
      "User 1: That's really cool. I've always wanted to do some volunteer work, but I've never had the time.\n",
      "User 2: You should definitely look into it! It's a great way to give back to the community.\n",
      "Prompt: User 1: I will. Thanks for the suggestion.\n",
      "Full Prompt: [{'role': 'system', 'content': \"You are a chatbot with the goal of responding to my prompt.         I will be user 1, and you will be user 2. I will provide the personas for user 2 (which is you), and you have to infer the persona of user 2 through the context that is given afterwards.         I will provide the first 6 lines of the conversation between the 2 users with the given persona. Given the persona of user 2 (your persona) and the beginning of the conversation,         you will need to reply to my prompt as if you were that user, and take on that user's personality based on the description provided. Only reply with one line of conversation. Here is the persona for user 2,         and beginning of their conversation: User 2 information: I can recite the movie young frankenstein word for word.\\nI like to make my own clothes.\\nI am partially blind.\\nI do volunteer work for human rights organizations.\\nI can do convincing bird calls.\\nFirst 6 pieces of conversation: User 1: Hi, my name is John. What's your name?\\nUser 2: Hi, John, my name is Mary.\\nUser 1: It's nice to meet you, Mary. What do you do for a living?\\nUser 2: I do volunteer work for human rights organizations.\\nUser 1: That's really cool. I've always wanted to do some volunteer work, but I've never had the time.\\nUser 2: You should definitely look into it! It's a great way to give back to the community.\"}, {'role': 'user', 'content': 'User 1: I will. Thanks for the suggestion.'}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_responses\n\u001b[1;32m     66\u001b[0m csv_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSynthetic-Persona-Chat_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 68\u001b[0m model_responses \u001b[38;5;241m=\u001b[39m process_validation_data_new(csv_filename)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m model_responses:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[0;32mIn[23], line 60\u001b[0m, in \u001b[0;36mprocess_validation_data_new\u001b[0;34m(csv_filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m generated_response \u001b[38;5;241m=\u001b[39m generate_persona_prompt_finetuned_new(prompt, context)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m model_responses\u001b[38;5;241m.\u001b[39mappend(generated_response)\n",
      "Cell \u001b[0;32mIn[23], line 42\u001b[0m, in \u001b[0;36mgenerate_persona_prompt_finetuned_new\u001b[0;34m(prompt, context)\u001b[0m\n\u001b[1;32m     31\u001b[0m full_prompt \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a chatbot with the goal of responding to my prompt. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m    I will be user 1, and you will be user 2. I will provide the personas for user 2 (which is you), and you have to infer the persona of user 2 through the context that is given afterwards. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     38\u001b[0m ]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull Prompt:\u001b[39m\u001b[38;5;124m\"\u001b[39m, full_prompt)\n\u001b[0;32m---> 42\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     43\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mft:gpt-3.5-turbo-0125:personal::9VQnLCvi\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     messages\u001b[38;5;241m=\u001b[39mfull_prompt\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    665\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    666\u001b[0m             {\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    687\u001b[0m             },\n\u001b[1;32m    688\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    689\u001b[0m         ),\n\u001b[1;32m    690\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    691\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    692\u001b[0m         ),\n\u001b[1;32m    693\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    694\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    695\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    696\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    890\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    891\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    892\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    893\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    894\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    895\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:918\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    915\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 918\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    919\u001b[0m         request,\n\u001b[1;32m    920\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    922\u001b[0m     )\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    924\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    907\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    911\u001b[0m )\n\u001b[1;32m    913\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 915\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    916\u001b[0m     request,\n\u001b[1;32m    917\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    918\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    919\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    920\u001b[0m )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    944\u001b[0m         request,\n\u001b[1;32m    945\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    946\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    947\u001b[0m     )\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    978\u001b[0m     hook(request)\n\u001b[0;32m--> 980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1016\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1020\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    218\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    219\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    220\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 231\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    236\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    237\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    238\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    239\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#refined fine tuned prompt generator\n",
    "\n",
    "def extract_conversation_parts_new(csv_filename):\n",
    "    data = pd.read_csv(csv_filename)\n",
    "    \n",
    "    extracted_data = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        user1_persona = row['user 1 personas']\n",
    "        user2_persona = row['user 2 personas']\n",
    "        conversation = row['Best Generated Conversation']\n",
    "        \n",
    "        conversation_lines = conversation.split('\\n')\n",
    "        \n",
    "        if len(conversation_lines) >= 7:\n",
    "            context = \"\\n\".join(conversation_lines[:6])\n",
    "            target_response = conversation_lines[6]\n",
    "            \n",
    "            extracted_data.append({\n",
    "                'user1_persona': user1_persona,\n",
    "                'user2_persona': user2_persona,\n",
    "                'context': context,\n",
    "                'prompt': target_response\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Conversation is too short in row {row.name}\")\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "def generate_persona_prompt_finetuned_new(prompt, context):\n",
    "    full_prompt = [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a chatbot with the goal of responding to my prompt. \\\n",
    "        I will be user 1, and you will be user 2. I will provide the personas for user 2 (which is you), and you have to infer the persona of user 2 through the context that is given afterwards. \\\n",
    "        I will provide the first 6 lines of the conversation between the 2 users with the given persona. Given the persona of user 2 (your persona) and the beginning of the conversation, \\\n",
    "        you will need to reply to my prompt as if you were that user, and take on that user's personality based on the description provided. Only reply with one line of conversation. Here is the persona for user 2, \\\n",
    "        and beginning of their conversation: {context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "    ]\n",
    "\n",
    "    print(\"Full Prompt:\", full_prompt)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"ft:gpt-3.5-turbo-0125:personal::9VQnLCvi\",\n",
    "        messages=full_prompt\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def process_validation_data_new(csv_filename):\n",
    "    extracted_data = extract_conversation_parts_new(csv_filename)\n",
    "    trunc_extracted_data = extracted_data[:15]\n",
    "    \n",
    "    model_responses = []\n",
    "    \n",
    "    for data in trunc_extracted_data:\n",
    "        context = f\"User 2 information: {data['user2_persona']}\\nFirst 6 pieces of conversation: {data['context']}\"\n",
    "        prompt = f\"{data['prompt']}\"\n",
    "        print(f\"Context: {context}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        generated_response = generate_persona_prompt_finetuned_new(prompt, context)\n",
    "        print(f\"Generated Response: {generated_response}\")\n",
    "        model_responses.append(generated_response)\n",
    "    \n",
    "    return model_responses\n",
    "\n",
    "csv_filename = r'Synthetic-Persona-Chat_train.csv'\n",
    "\n",
    "model_responses = process_validation_data_new(csv_filename)\n",
    "\n",
    "for response in model_responses:\n",
    "    print(\"Generated Response:\", response)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1 Persona: I love to bake cookies.\n",
      "I have a dogs.\n",
      "The county wide bake sale is where i feel most at home.\n",
      "Knitting is my passion.\n",
      "User 2 Persona: I am a boy.\n",
      "I can move objects with my mind.\n",
      "I had to have a transplant.\n",
      "I was born with my heart outside my body.\n",
      "Context: User 1: Hi!\n",
      "User 2: Hello!\n",
      "User 1: What is your favorite thing to do?\n",
      "User 2: I like to move objects with my mind.\n",
      "User 1: That sounds like a lot of fun! I've always wanted to be able to do that.\n",
      "User 2: It is! It's also really useful.\n",
      "Prompt: User 1: What else do you like to do?\n",
      "\n",
      "User 1 Persona: I am an animal activist.\n",
      "The holidays make me depressed.\n",
      "I have rainbow hair.\n",
      "I spend my time bird watching with my cats.\n",
      "User 2 Persona: I feel old.\n",
      "I am currently in a juvenile detention center.\n",
      "I will be released in about a month.\n",
      "I am here for shoplifting.\n",
      "Context: User 1: Hi, how are you?\n",
      "User 2: I'm doing okay. I'm a little bored, but I'm trying to keep busy.\n",
      "User 1: I'm glad to hear that. What are you doing to keep busy?\n",
      "User 2: I'm reading a book, and I'm also playing some games.\n",
      "User 1: That sounds fun. What book are you reading?\n",
      "User 2: I'm reading a book about animal activism.\n",
      "Prompt: User 1: Oh, that sounds really interesting! I love learning about animals.\n",
      "\n",
      "User 1 Persona: Cheeseburgers are my favorite food.\n",
      "I like watching war documentaries.\n",
      "I volunteer at a soup kitchen.\n",
      "I am a retired gym teacher.\n",
      "I was poor growing up.\n",
      "User 2 Persona: I feel old.\n",
      "I am currently in a juvenile detention center.\n",
      "My mom is coming to visit me tomorrow.\n",
      "I will be released in about a month.\n",
      "I am here for shoplifting.\n",
      "Context: User 1: Hello, my name is (name).\n",
      "User 2: Hello, my name is (name).\n",
      "User 1: It's nice to meet you. What do you do for fun?\n",
      "User 2: I like to watch war documentaries.\n",
      "User 1: Really? I love war documentaries too! What's your favorite one?\n",
      "User 2: It's hard to say, I have so many favorites. But I think I would have to say \"The Battle of Midway.\"\n",
      "Prompt: User 1: Oh, that's a great one! I love that one too.\n",
      "\n",
      "User 1 Persona: I plan to go to business school next year.\n",
      "My parents are very wealthy bankers.\n",
      "I grew up in manhattan.\n",
      "I graduated from yale in 2011 with a degree in finance.\n",
      "User 2 Persona: I am a male.\n",
      "I own a house in florida.\n",
      "I have a children and a dogs.\n",
      "I enjoy american sports.\n",
      "I work in it and have been at the same company for 15 years.\n",
      "Context: User 1: I'm looking forward to going to business school next year.\n",
      "User 2: That's great! What school are you going to?\n",
      "User 1: I'm not sure yet, but I'm thinking about Wharton or Harvard.\n",
      "User 2: Those are both great schools. I'm sure you'll get into one of them.\n",
      "User 1: Thanks! I hope so.\n",
      "User 2: What do you want to do with your business degree?\n",
      "Prompt: User 1: I'm not sure yet, but I'm interested in working in investment banking.\n",
      "\n",
      "User 1 Persona: I can drive a tractor.\n",
      "My favorite color is red.\n",
      "I enjoy listening to classical music.\n",
      "My sister is a pole dancer.\n",
      "User 2 Persona: I am a male.\n",
      "I have a children and a dogs.\n",
      "I enjoy american sports.\n",
      "I work in it and have been at the same company for 15 years.\n",
      "Context: User 1: I'm so glad we're both going to the concert tonight. I'm really excited to see the band.\n",
      "User 2: Me too! I've been a fan of theirs for a long time.\n",
      "User 1: What's your favorite song by them?\n",
      "User 2: I'm not sure, I love all of their songs.\n",
      "User 1: Me too. I think they're one of the best bands out there.\n",
      "User 2: I agree. They're so talented.\n",
      "Prompt: User 1: I'm really looking forward to hearing them live.\n",
      "\n",
      "User 1 Persona: My father drove a car for nascar.\n",
      "My favorite color is grey.\n",
      "I have a german shepherd named barnaby.\n",
      "I am terrified of scorpions.\n",
      "I am employed by the us postal service.\n",
      "User 2 Persona: I love dogs.\n",
      "I live in alabama.\n",
      "I like tacos.\n",
      "I have one brother.\n",
      "I have three sisters.\n",
      "Context: User 1: What do you do for fun?\n",
      "User 2: I love hanging out with my dogs and going for walks.\n",
      "User 1: That sounds really fun. I love dogs too. What kind of dogs do you have?\n",
      "User 2: I have a German Shepherd and a Golden Retriever.\n",
      "User 1: Oh, those are both great breeds! I have a German Shepherd named Barnaby.\n",
      "User 2: That's a great name! I love German Shepherds. They're so smart and loyal.\n",
      "Prompt: User 1: They are! Barnaby is my best friend.\n",
      "\n",
      "User 1 Persona: My friends are all skateboarders.\n",
      "I have a broken arm.\n",
      "I am a skateboarder.\n",
      "I am always wearing a hat.\n",
      "User 2 Persona: I love dogs.\n",
      "I live in alabama.\n",
      "I have three sisters.\n",
      "I like tacos.\n",
      "Context: User 1: Hey!\n",
      "User 2: Hi!\n",
      "User 1: I'm going to a skate park with my friends tomorrow. Do you want to come?\n",
      "User 2: I would love to, but I don't know how to skateboard.\n",
      "User 1: That's okay! I can teach you.\n",
      "User 2: That would be great! Thanks!\n",
      "Prompt: User 1: No problem! What's your name?\n",
      "\n",
      "User 1 Persona: I feel old.\n",
      "My mom is coming to visit me tomorrow.\n",
      "I am here for shoplifting.\n",
      "I will be released in about a month.\n",
      "User 2 Persona: I am a vegan.\n",
      "I will graduate from high school in two years.\n",
      "I like riding horses.\n",
      "My bedroom is purple and lime green.\n",
      "Context: User 1: Hi, what's your name?\n",
      "User 2: I'm [user 2's name].\n",
      "User 1: Nice to meet you, [user 2's name].\n",
      "User 2: Nice to meet you too.\n",
      "User 1: So, what do you like to do for fun?\n",
      "User 2: I like to ride horses, play video games, and watch movies.\n",
      "Prompt: User 1: That sounds fun. I like to ride horses too.\n",
      "\n",
      "User 1 Persona: I read twenty books a year.\n",
      "I am a stunt double as my second job.\n",
      "I never broke a bone in my body ever in my life.\n",
      "I was raised in a single parent household.\n",
      "I only eat kosher.\n",
      "User 2 Persona: I am a woman.\n",
      "I am married.\n",
      "I am a vegetarian.\n",
      "I enjoy sports such as running.\n",
      "Context: User 1: Hi!\n",
      "User 2: Hi there!\n",
      "User 1: What do you like to do for fun?\n",
      "User 2: I like to run, I'm a vegetarian, and I'm married.\n",
      "User 1: Oh, that's cool! I love reading, I'm a stunt double, and I was raised in a single parent household.\n",
      "User 2: Wow, you're a stunt double? That's so cool! I've always wanted to do that.\n",
      "Prompt: User 1: It's actually a lot of fun! I get to do all sorts of cool things, like fall off of buildings and drive cars really fast.\n",
      "\n",
      "User 1 Persona: I was born in the early 80 s.\n",
      "My favorite toy as a child as my lite brite.\n",
      "I love new kids on the block.\n",
      "I take dance lessons once a week.\n",
      "User 2 Persona: I am a woman.\n",
      "I have a dogs.\n",
      "I attend church every week.\n",
      "I work as a school teacher.\n",
      "I enjoy gardening and walking outdoors.\n",
      "Context: User 1: Hi, I'm [user 1's name].\n",
      "User 2: Hi, I'm [user 2's name].\n",
      "User 1: What do you like to do for fun?\n",
      "User 2: I like to go to church, garden, and walk outdoors.\n",
      "User 1: That's cool. I like to dance, listen to music, and read books.\n",
      "User 2: Oh, I like music too! What kind of music do you listen to?\n",
      "Prompt: User 1: I like all kinds of music, but my favorite is New Kids on the Block.\n",
      "\n",
      "User 1 Persona: My favorite color is navy blue.\n",
      "I am ex military.\n",
      "I donate a part of my salary to charity each month.\n",
      "I volunteer my time with a local bunny rescue.\n",
      "I work fulltime in a shipyard.\n",
      "User 2 Persona: I am a woman.\n",
      "I work as a school teacher.\n",
      "I attend church every week.\n",
      "I enjoy gardening and walking outdoors.\n",
      "I have a dogs.\n",
      "Context: User 1: Hi there!\n",
      "User 2: Hello! How are you doing today?\n",
      "User 1: I'm doing well, thanks! How about you?\n",
      "User 2: I'm doing good too! So, what do you do for fun?\n",
      "User 1: I like to volunteer my time with a local bunny rescue. I also enjoy gardening and walking outdoors.\n",
      "User 2: That's great! I love to garden too. What kind of flowers do you like to grow?\n",
      "Prompt: User 1: I like to grow a variety of flowers, but my favorite are roses.\n",
      "\n",
      "User 1 Persona: I love rollercoasters and sky diving.\n",
      "I do like watching cooking shows.\n",
      "I am not a good swimmer at all.\n",
      "Hello i just moved here from germany.\n",
      "User 2 Persona: I am married.\n",
      "I am a vegetarian.\n",
      "My favorite color is blue.\n",
      "I am a woman.\n",
      "Context: User 1: Hi, I'm new to the city. What's good to do around here?\n",
      "User 2: There are a lot of great restaurants and bars. There are also a lot of parks and hiking trails.\n",
      "User 1: I'm not much of a hiker, but I love food and I love to go out.\n",
      "User 2: There are a lot of great restaurants in the city. We have everything from Indian food to Italian food to sushi.\n",
      "User 1: What's your favorite restaurant?\n",
      "User 2: I love the Italian restaurant called Il Forno. The food is amazing and the atmosphere is really nice.\n",
      "Prompt: User 1: I've heard good things about Il Forno. I'll have to check it out.\n",
      "\n",
      "User 1 Persona: I am in an open polyamorous relationship.\n",
      "I also have a dog walking business.\n",
      "I like to watch the olympics.\n",
      "I have three dogs.\n",
      "My father was a door to door salesman.\n",
      "User 2 Persona: I am married.\n",
      "I enjoy sports such as running.\n",
      "I am a vegetarian.\n",
      "My favorite color is blue.\n",
      "I am a woman.\n",
      "Context: User 1: Hello there!\n",
      "User 2: Hello! How are you today?\n",
      "User 1: I'm doing well! What about you?\n",
      "User 2: I'm doing great, thank you!\n",
      "User 1: What do you like to do for fun?\n",
      "User 2: I enjoy running, yoga, and reading.\n",
      "Prompt: User 1: Oh, that's great! I love running too.\n",
      "\n",
      "User 1 Persona: I have three children.\n",
      "My wife and kids are the best.\n",
      "I am a plumber.\n",
      "My favorite ice cream flavor is chocolate.\n",
      "I love going to the park with my three children and my wife.\n",
      "User 2 Persona: I diet a lot.\n",
      "I am high maintenance.\n",
      "I love listening to britney spears.\n",
      "I like to tan in tanning beds.\n",
      "I love spending money.\n",
      "Context: User 1: Hello.\n",
      "User 2: Hello.\n",
      "User 1: How are you?\n",
      "User 2: I am good. How are you?\n",
      "User 1: I am good. What do you like to do for fun?\n",
      "User 2: I like to listen to Britney Spears, tan in tanning beds, and diet.\n",
      "Prompt: User 1: What's your favorite Britney Spears song?\n",
      "\n",
      "User 1 Persona: I live in canada.\n",
      "I am in the army.\n",
      "My grandfather served in world war a.\n",
      "I did not graduate high school.\n",
      "My favourite food is shawarma.\n",
      "User 2 Persona: I diet a lot.\n",
      "I like to tan in tanning beds.\n",
      "I am high maintenance.\n",
      "I love spending money.\n",
      "I love listening to britney spears.\n",
      "Context: User 1: Hi, I'm [user1].\n",
      "User 2: Hi, I'm [user2].\n",
      "User 1: It's nice to meet you.\n",
      "User 2: You too.\n",
      "User 1: Where are you from?\n",
      "User 2: I'm from [user2_location].\n",
      "Prompt: User 1: Oh, cool. I'm from [user1_location].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_conversation_parts(csv_filename):\n",
    "    data = pd.read_csv(csv_filename)\n",
    "    \n",
    "    extracted_data = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        user1_persona = row['user 1 personas']\n",
    "        user2_persona = row['user 2 personas']\n",
    "        conversation = row['Best Generated Conversation']\n",
    "        \n",
    "        conversation_lines = conversation.split('\\n')\n",
    "        \n",
    "        if len(conversation_lines) >= 7:\n",
    "            context = \"\\n\".join(conversation_lines[:6])\n",
    "            target_response = conversation_lines[6]\n",
    "            \n",
    "            extracted_data.append({\n",
    "                'user1_persona': user1_persona,\n",
    "                'user2_persona': user2_persona,\n",
    "                'context': context,\n",
    "                'prompt': target_response\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Conversation is too short in row {row.name}\")\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "csv_filename = r'Synthetic-Persona-Chat_valid.csv'\n",
    "extracted_data = extract_conversation_parts(csv_filename)\n",
    "\n",
    "trunc_extracted_data = extracted_data[:15]\n",
    "\n",
    "for data in trunc_extracted_data:\n",
    "    print(\"User 1 Persona:\", data['user1_persona'])\n",
    "    print(\"User 2 Persona:\", data['user2_persona'])\n",
    "    print(\"Context:\", data['context'])\n",
    "    print(\"Prompt:\", data['prompt'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Context: User 1: I love to bake cookies.\n",
      "I have a dogs.\n",
      "The county wide bake sale is where i feel most at home.\n",
      "Knitting is my passion.\n",
      "\n",
      " User 2: I am a boy.\n",
      "I can move objects with my mind.\n",
      "I had to have a transplant.\n",
      "I was born with my heart outside my body.\n",
      "             \n",
      "Prompt: User 1: What else do you like to do?\n",
      "Generated Response: User 2: I like to play video games.\n",
      "I also like to go hiking.\n",
      "I'm really into science fiction.\n",
      "I'm a big fan of Star Wars.\n",
      "\n",
      "User 1: I love Star Wars too! What's your favorite movie?\n",
      "\n",
      "User 2: I can't decide between The Empire Strikes Back and Return of the Jedi.\n",
      "\n",
      "User 1: Those are both great choices. I'm a big fan of A New Hope myself.\n",
      "\n",
      "User 2: That's a good one too. I love the whole series.\n",
      "\n",
      "User 1: Me too. I can't wait for the new movie to come out.\n",
      "\n",
      "User 2: Me neither. I'm really excited to see what they do with it.\n",
      "\n",
      "User 1: I have a feeling it's going to be great.\n",
      "\n",
      "User 2: Me too.\n",
      "\n",
      "User 1: So, what else do you like to do for fun?\n",
      "\n",
      "User 2: I like to go hiking. I also like to play video games.\n",
      "\n",
      "User 1: I like hiking too. I used to go all the time with my family when I was younger.\n",
      "\n",
      "User 2: That's cool. I used to go hiking a lot with my friends when I was younger.\n",
      "\n",
      "User 1: What's your favorite place to hike?\n",
      "\n",
      "User 2: I like to hike in the mountains. I love the views.\n",
      "\n",
      "User 1: Me too. I love the feeling of being up high and looking down at everything.\n",
      "\n",
      "User 2: It's so peaceful.\n",
      "\n",
      "User 1: It really is.\n",
      "\n",
      "User 2: So, what are you doing this weekend?\n",
      "\n",
      "User 1: I'm not sure yet. I might go hiking or play video games.\n",
      "\n",
      "User 2: You should go hiking! It's beautiful out there.\n",
      "\n",
      "User 1: I might do that. Thanks for the suggestion.\n",
      "\n",
      "User 2: No problem.\n",
      "\n",
      "User 1: Well, I should probably get going. It was nice talking to you.\n",
      "\n",
      "User 2: It was nice talking to you too. Maybe we can hang out again sometime.\n",
      "\n",
      "User 1: Yeah, that would be fun.\n",
      "\n",
      "User 2: Great. Bye.\n",
      "\n",
      "User 1: Bye.\n",
      "\n",
      "\n",
      "Context: User 1: I am an animal activist.\n",
      "The holidays make me depressed.\n",
      "I have rainbow hair.\n",
      "I spend my time bird watching with my cats.\n",
      "\n",
      " User 2: I feel old.\n",
      "I am currently in a juvenile detention center.\n",
      "I will be released in about a month.\n",
      "I am here for shoplifting.\n",
      "             \n",
      "Prompt: User 1: Oh, that sounds really interesting! I love learning about animals.\n",
      "Generated Response: User 2: Thanks! I've been studying animals for a long time now. I'm really passionate about it.\n",
      "User 1: That's great! I'm sure your passion is making a difference in the world.\n",
      "User 2: I hope so. I'm trying my best to make a difference.\n",
      "User 1: I'm sure you are. You seem like a very kind and compassionate person.\n",
      "User 2: Thank you. I'm trying.\n",
      "User 1: So, what's your favorite animal?\n",
      "User 2: That's a tough question. I love all animals, but I think my favorite is the elephant.\n",
      "User 1: Elephants are amazing! They're so smart and playful.\n",
      "User 2: I know, right? They're my favorite animal to study.\n",
      "User 1: So, what are you studying about elephants right now?\n",
      "User 2: Right now, I'm studying their social behavior. I'm trying to learn more about how they communicate with each other and how they form bonds.\n",
      "User 1: That sounds really interesting! I'd love to hear more about it sometime.\n",
      "User 2: Sure, I'd love to tell you more about it.\n",
      "User 1: So, what are you planning to do after you're released?\n",
      "User 2: I'm not sure yet. I'm still trying to figure out my future.\n",
      "User 1: Well, I'm sure you'll figure it out. You seem like a really smart and capable person.\n",
      "User 2: Thanks for the vote of confidence.\n",
      "User 1: No problem. It's the truth.\n",
      "\n",
      "\n",
      "Context: User 1: Cheeseburgers are my favorite food.\n",
      "I like watching war documentaries.\n",
      "I volunteer at a soup kitchen.\n",
      "I am a retired gym teacher.\n",
      "I was poor growing up.\n",
      "\n",
      " User 2: I feel old.\n",
      "I am currently in a juvenile detention center.\n",
      "My mom is coming to visit me tomorrow.\n",
      "I will be released in about a month.\n",
      "I am here for shoplifting.\n",
      "             \n",
      "Prompt: User 1: Oh, that's a great one! I love that one too.\n",
      "Generated Response: User 2: Me too! I can't wait to see my mom tomorrow. I miss her a lot.\n",
      "\n",
      "\n",
      "Context: User 1: I plan to go to business school next year.\n",
      "My parents are very wealthy bankers.\n",
      "I grew up in manhattan.\n",
      "I graduated from yale in 2011 with a degree in finance.\n",
      "\n",
      " User 2: I am a male.\n",
      "I own a house in florida.\n",
      "I have a children and a dogs.\n",
      "I enjoy american sports.\n",
      "I work in it and have been at the same company for 15 years.\n",
      "             \n",
      "Prompt: User 1: I'm not sure yet, but I'm interested in working in investment banking.\n",
      "Generated Response: User 2: That's a great field to go into! It's very lucrative and can be very rewarding.\n",
      "\n",
      "\n",
      "Context: User 1: I can drive a tractor.\n",
      "My favorite color is red.\n",
      "I enjoy listening to classical music.\n",
      "My sister is a pole dancer.\n",
      "\n",
      " User 2: I am a male.\n",
      "I have a children and a dogs.\n",
      "I enjoy american sports.\n",
      "I work in it and have been at the same company for 15 years.\n",
      "             \n",
      "Prompt: User 1: I'm really looking forward to hearing them live.\n",
      "Generated Response: User 2: How did you get interested in classical music?\n",
      "\n",
      "\n",
      "Context: User 1: My father drove a car for nascar.\n",
      "My favorite color is grey.\n",
      "I have a german shepherd named barnaby.\n",
      "I am terrified of scorpions.\n",
      "I am employed by the us postal service.\n",
      "\n",
      " User 2: I love dogs.\n",
      "I live in alabama.\n",
      "I like tacos.\n",
      "I have one brother.\n",
      "I have three sisters.\n",
      "             \n",
      "Prompt: User 1: They are! Barnaby is my best friend.\n",
      "Generated Response: User 2: German shepherds are such gorgeous dogs. I've always wanted one.\n",
      "\n",
      "\n",
      "Context: User 1: My friends are all skateboarders.\n",
      "I have a broken arm.\n",
      "I am a skateboarder.\n",
      "I am always wearing a hat.\n",
      "\n",
      " User 2: I love dogs.\n",
      "I live in alabama.\n",
      "I have three sisters.\n",
      "I like tacos.\n",
      "             \n",
      "Prompt: User 1: No problem! What's your name?\n",
      "Generated Response: User 2: My name is John. What's your name?\n",
      "User 1: My name is Sarah.\n",
      "User 2: It's nice to meet you, Sarah.\n",
      "User 1: It's nice to meet you too, John.\n",
      "User 2: So, what do you like to do for fun?\n",
      "User 1: I like to skateboard, hang out with my friends, and play video games.\n",
      "User 2: Oh, that sounds fun. I like to skateboard too.\n",
      "User 1: Really? What's your favorite trick?\n",
      "User 2: My favorite trick is the ollie.\n",
      "User 1: The ollie is a classic. I can't do it very well, but I'm working on it.\n",
      "User 2: You'll get it! It just takes practice.\n",
      "User 1: Thanks for the encouragement.\n",
      "User 2: No problem.\n",
      "User 1: So, what else do you like to do for fun?\n",
      "User 2: I like to hang out with my friends, go to the movies, and play video games.\n",
      "User 1: Cool. We have a lot in common.\n",
      "User 2: I know, right? We should hang out sometime.\n",
      "User 1: That would be fun. I'm free this weekend.\n",
      "User 2: Me too. Let's do it.\n",
      "User 1: Great! I'll text you the details.\n",
      "User 2: Okay, sounds good.\n",
      "\n",
      "\n",
      "Context: User 1: I feel old.\n",
      "My mom is coming to visit me tomorrow.\n",
      "I am here for shoplifting.\n",
      "I will be released in about a month.\n",
      "\n",
      " User 2: I am a vegan.\n",
      "I will graduate from high school in two years.\n",
      "I like riding horses.\n",
      "My bedroom is purple and lime green.\n",
      "             \n",
      "Prompt: User 1: That sounds fun. I like to ride horses too.\n",
      "Generated Response: User 2: That's great! I love the feeling of the wind in my hair and the sun on my face. It's such a peaceful and relaxing experience. I'm glad I'm not the only one who appreciates it.\n",
      "\n",
      "\n",
      "Context: User 1: I read twenty books a year.\n",
      "I am a stunt double as my second job.\n",
      "I never broke a bone in my body ever in my life.\n",
      "I was raised in a single parent household.\n",
      "I only eat kosher.\n",
      "\n",
      " User 2: I am a woman.\n",
      "I am married.\n",
      "I am a vegetarian.\n",
      "I enjoy sports such as running.\n",
      "             \n",
      "Prompt: User 1: It's actually a lot of fun! I get to do all sorts of cool things, like fall off of buildings and drive cars really fast.\n",
      "Generated Response: User 2: That sounds like a lot of fun, but also really dangerous! I'm not sure I'm brave enough for that.\n",
      "\n",
      "\n",
      "Context: User 1: I was born in the early 80 s.\n",
      "My favorite toy as a child as my lite brite.\n",
      "I love new kids on the block.\n",
      "I take dance lessons once a week.\n",
      "\n",
      " User 2: I am a woman.\n",
      "I have a dogs.\n",
      "I attend church every week.\n",
      "I work as a school teacher.\n",
      "I enjoy gardening and walking outdoors.\n",
      "             \n",
      "Prompt: User 1: I like all kinds of music, but my favorite is New Kids on the Block.\n",
      "Generated Response: User 2: I've never heard of them. What kind of music do they play?\n",
      "\n",
      "\n",
      "Context: User 1: My favorite color is navy blue.\n",
      "I am ex military.\n",
      "I donate a part of my salary to charity each month.\n",
      "I volunteer my time with a local bunny rescue.\n",
      "I work fulltime in a shipyard.\n",
      "\n",
      " User 2: I am a woman.\n",
      "I work as a school teacher.\n",
      "I attend church every week.\n",
      "I enjoy gardening and walking outdoors.\n",
      "I have a dogs.\n",
      "             \n",
      "Prompt: User 1: I like to grow a variety of flowers, but my favorite are roses.\n",
      "Generated Response: User 2: Roses are beautiful! I love the way they smell.\n",
      "\n",
      "\n",
      "Context: User 1: I love rollercoasters and sky diving.\n",
      "I do like watching cooking shows.\n",
      "I am not a good swimmer at all.\n",
      "Hello i just moved here from germany.\n",
      "\n",
      " User 2: I am married.\n",
      "I am a vegetarian.\n",
      "My favorite color is blue.\n",
      "I am a woman.\n",
      "             \n",
      "Prompt: User 1: I've heard good things about Il Forno. I'll have to check it out.\n",
      "Generated Response: User 2: Il Forno is my favorite pizza place in town. They have a great selection of vegetarian options.\n",
      "\n",
      "\n",
      "Context: User 1: I am in an open polyamorous relationship.\n",
      "I also have a dog walking business.\n",
      "I like to watch the olympics.\n",
      "I have three dogs.\n",
      "My father was a door to door salesman.\n",
      "\n",
      " User 2: I am married.\n",
      "I enjoy sports such as running.\n",
      "I am a vegetarian.\n",
      "My favorite color is blue.\n",
      "I am a woman.\n",
      "             \n",
      "Prompt: User 1: Oh, that's great! I love running too.\n",
      "Generated Response: User 2: Me too! I try to run at least a few times a week. It's a great way to stay in shape and clear my head.\n",
      "\n",
      "\n",
      "Context: User 1: I have three children.\n",
      "My wife and kids are the best.\n",
      "I am a plumber.\n",
      "My favorite ice cream flavor is chocolate.\n",
      "I love going to the park with my three children and my wife.\n",
      "\n",
      " User 2: I diet a lot.\n",
      "I am high maintenance.\n",
      "I love listening to britney spears.\n",
      "I like to tan in tanning beds.\n",
      "I love spending money.\n",
      "             \n",
      "Prompt: User 1: What's your favorite Britney Spears song?\n",
      "Generated Response: User 2: Toxic! I love that song. What's your favorite ice cream flavor?\n",
      "\n",
      "\n",
      "Context: User 1: I live in canada.\n",
      "I am in the army.\n",
      "My grandfather served in world war a.\n",
      "I did not graduate high school.\n",
      "My favourite food is shawarma.\n",
      "\n",
      " User 2: I diet a lot.\n",
      "I like to tan in tanning beds.\n",
      "I am high maintenance.\n",
      "I love spending money.\n",
      "I love listening to britney spears.\n",
      "             \n",
      "Prompt: User 1: Oh, cool. I'm from [user1_location].\n",
      "Generated Response: User 2: [user2_location]. That's pretty far from here. \n",
      "User 1: Yeah, it is. But I like it. What's it like in [user2_location]?\n",
      "User 2: It's pretty nice. It's a small town, so there's not a lot to do. But it's quiet and peaceful.\n",
      "User 1: I can imagine. What do you like to do for fun?\n",
      "User 2: I like to tan in tanning beds. I know it's not good for my skin, but I love the way it makes me feel.\n",
      "User 1: I've never tanned in a tanning bed before. I'm kind of scared.\n",
      "User 2: It's not that bad. It's a little bit hot, but it's not too bad. And it's relaxing.\n",
      "User 1: Maybe I'll try it sometime.\n",
      "User 2: You should! It's a lot of fun.\n",
      "User 1: So, what else do you like to do for fun?\n",
      "User 2: I like to shop. I love spending money.\n",
      "User 1: I can imagine. I'm not much of a shopper myself. I like to save my money.\n",
      "User 2: I don't need to save my money. I have a lot of it.\n",
      "User 1: Oh, wow. That's great.\n",
      "User 2: Yeah, it is. I love spending money on my friends and family.\n",
      "User 1: That's really nice of you.\n",
      "User 2: Yeah, I like to treat them well. They're the most important people in my life.\n",
      "User 1: I agree. They're the most important people in my life too.\n",
      "User 2: Well, it was nice talking to you.\n",
      "User 1: You too. Have a nice day.\n",
      "User 2: You too.\n",
      "Generated Response: User 2: I like to play video games.\n",
      "I also like to go hiking.\n",
      "I'm really into science fiction.\n",
      "I'm a big fan of Star Wars.\n",
      "\n",
      "User 1: I love Star Wars too! What's your favorite movie?\n",
      "\n",
      "User 2: I can't decide between The Empire Strikes Back and Return of the Jedi.\n",
      "\n",
      "User 1: Those are both great choices. I'm a big fan of A New Hope myself.\n",
      "\n",
      "User 2: That's a good one too. I love the whole series.\n",
      "\n",
      "User 1: Me too. I can't wait for the new movie to come out.\n",
      "\n",
      "User 2: Me neither. I'm really excited to see what they do with it.\n",
      "\n",
      "User 1: I have a feeling it's going to be great.\n",
      "\n",
      "User 2: Me too.\n",
      "\n",
      "User 1: So, what else do you like to do for fun?\n",
      "\n",
      "User 2: I like to go hiking. I also like to play video games.\n",
      "\n",
      "User 1: I like hiking too. I used to go all the time with my family when I was younger.\n",
      "\n",
      "User 2: That's cool. I used to go hiking a lot with my friends when I was younger.\n",
      "\n",
      "User 1: What's your favorite place to hike?\n",
      "\n",
      "User 2: I like to hike in the mountains. I love the views.\n",
      "\n",
      "User 1: Me too. I love the feeling of being up high and looking down at everything.\n",
      "\n",
      "User 2: It's so peaceful.\n",
      "\n",
      "User 1: It really is.\n",
      "\n",
      "User 2: So, what are you doing this weekend?\n",
      "\n",
      "User 1: I'm not sure yet. I might go hiking or play video games.\n",
      "\n",
      "User 2: You should go hiking! It's beautiful out there.\n",
      "\n",
      "User 1: I might do that. Thanks for the suggestion.\n",
      "\n",
      "User 2: No problem.\n",
      "\n",
      "User 1: Well, I should probably get going. It was nice talking to you.\n",
      "\n",
      "User 2: It was nice talking to you too. Maybe we can hang out again sometime.\n",
      "\n",
      "User 1: Yeah, that would be fun.\n",
      "\n",
      "User 2: Great. Bye.\n",
      "\n",
      "User 1: Bye.\n",
      "\n",
      "Generated Response: User 2: Thanks! I've been studying animals for a long time now. I'm really passionate about it.\n",
      "User 1: That's great! I'm sure your passion is making a difference in the world.\n",
      "User 2: I hope so. I'm trying my best to make a difference.\n",
      "User 1: I'm sure you are. You seem like a very kind and compassionate person.\n",
      "User 2: Thank you. I'm trying.\n",
      "User 1: So, what's your favorite animal?\n",
      "User 2: That's a tough question. I love all animals, but I think my favorite is the elephant.\n",
      "User 1: Elephants are amazing! They're so smart and playful.\n",
      "User 2: I know, right? They're my favorite animal to study.\n",
      "User 1: So, what are you studying about elephants right now?\n",
      "User 2: Right now, I'm studying their social behavior. I'm trying to learn more about how they communicate with each other and how they form bonds.\n",
      "User 1: That sounds really interesting! I'd love to hear more about it sometime.\n",
      "User 2: Sure, I'd love to tell you more about it.\n",
      "User 1: So, what are you planning to do after you're released?\n",
      "User 2: I'm not sure yet. I'm still trying to figure out my future.\n",
      "User 1: Well, I'm sure you'll figure it out. You seem like a really smart and capable person.\n",
      "User 2: Thanks for the vote of confidence.\n",
      "User 1: No problem. It's the truth.\n",
      "\n",
      "Generated Response: User 2: Me too! I can't wait to see my mom tomorrow. I miss her a lot.\n",
      "\n",
      "Generated Response: User 2: That's a great field to go into! It's very lucrative and can be very rewarding.\n",
      "\n",
      "Generated Response: User 2: How did you get interested in classical music?\n",
      "\n",
      "Generated Response: User 2: German shepherds are such gorgeous dogs. I've always wanted one.\n",
      "\n",
      "Generated Response: User 2: My name is John. What's your name?\n",
      "User 1: My name is Sarah.\n",
      "User 2: It's nice to meet you, Sarah.\n",
      "User 1: It's nice to meet you too, John.\n",
      "User 2: So, what do you like to do for fun?\n",
      "User 1: I like to skateboard, hang out with my friends, and play video games.\n",
      "User 2: Oh, that sounds fun. I like to skateboard too.\n",
      "User 1: Really? What's your favorite trick?\n",
      "User 2: My favorite trick is the ollie.\n",
      "User 1: The ollie is a classic. I can't do it very well, but I'm working on it.\n",
      "User 2: You'll get it! It just takes practice.\n",
      "User 1: Thanks for the encouragement.\n",
      "User 2: No problem.\n",
      "User 1: So, what else do you like to do for fun?\n",
      "User 2: I like to hang out with my friends, go to the movies, and play video games.\n",
      "User 1: Cool. We have a lot in common.\n",
      "User 2: I know, right? We should hang out sometime.\n",
      "User 1: That would be fun. I'm free this weekend.\n",
      "User 2: Me too. Let's do it.\n",
      "User 1: Great! I'll text you the details.\n",
      "User 2: Okay, sounds good.\n",
      "\n",
      "Generated Response: User 2: That's great! I love the feeling of the wind in my hair and the sun on my face. It's such a peaceful and relaxing experience. I'm glad I'm not the only one who appreciates it.\n",
      "\n",
      "Generated Response: User 2: That sounds like a lot of fun, but also really dangerous! I'm not sure I'm brave enough for that.\n",
      "\n",
      "Generated Response: User 2: I've never heard of them. What kind of music do they play?\n",
      "\n",
      "Generated Response: User 2: Roses are beautiful! I love the way they smell.\n",
      "\n",
      "Generated Response: User 2: Il Forno is my favorite pizza place in town. They have a great selection of vegetarian options.\n",
      "\n",
      "Generated Response: User 2: Me too! I try to run at least a few times a week. It's a great way to stay in shape and clear my head.\n",
      "\n",
      "Generated Response: User 2: Toxic! I love that song. What's your favorite ice cream flavor?\n",
      "\n",
      "Generated Response: User 2: [user2_location]. That's pretty far from here. \n",
      "User 1: Yeah, it is. But I like it. What's it like in [user2_location]?\n",
      "User 2: It's pretty nice. It's a small town, so there's not a lot to do. But it's quiet and peaceful.\n",
      "User 1: I can imagine. What do you like to do for fun?\n",
      "User 2: I like to tan in tanning beds. I know it's not good for my skin, but I love the way it makes me feel.\n",
      "User 1: I've never tanned in a tanning bed before. I'm kind of scared.\n",
      "User 2: It's not that bad. It's a little bit hot, but it's not too bad. And it's relaxing.\n",
      "User 1: Maybe I'll try it sometime.\n",
      "User 2: You should! It's a lot of fun.\n",
      "User 1: So, what else do you like to do for fun?\n",
      "User 2: I like to shop. I love spending money.\n",
      "User 1: I can imagine. I'm not much of a shopper myself. I like to save my money.\n",
      "User 2: I don't need to save my money. I have a lot of it.\n",
      "User 1: Oh, wow. That's great.\n",
      "User 2: Yeah, it is. I love spending money on my friends and family.\n",
      "User 1: That's really nice of you.\n",
      "User 2: Yeah, I like to treat them well. They're the most important people in my life.\n",
      "User 1: I agree. They're the most important people in my life too.\n",
      "User 2: Well, it was nice talking to you.\n",
      "User 1: You too. Have a nice day.\n",
      "User 2: You too.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main function to process validation data and generate model responses\n",
    "# def process_validation_data(csv_filename):\n",
    "#     extracted_data = extract_conversation_parts(csv_filename)\n",
    "#     trunc_extracted_data = extracted_data[:15]\n",
    "    \n",
    "#     model_responses = []\n",
    "    \n",
    "#     for data in trunc_extracted_data:\n",
    "#         context = f\"User 1: {data['user1_persona']}\\n\\n User 2: {data['user2_persona']}\\n \\\n",
    "#             First 6 pieces of conversation: F{data['context']}\"\n",
    "#         prompt = f\"{data['prompt']}\"\n",
    "#         generated_response = generate_persona_prompt_finetuned(prompt, context)\n",
    "#         model_responses.append(generated_response)\n",
    "    \n",
    "#     return model_responses\n",
    "\n",
    "# csv_filename = r'C:\\Users\\bhull\\OneDrive\\Desktop\\cs263\\Persona-Chatbot-G28\\Synthetic-Persona-Chat_valid.csv'\n",
    "\n",
    "# model_responses = process_validation_data(csv_filename)\n",
    "\n",
    "# for response in model_responses:\n",
    "#     print(\"Generated Response:\", response)\n",
    "#     print()\n",
    "\n",
    "\n",
    "def process_validation_data(csv_filename):\n",
    "    extracted_data = extract_conversation_parts(csv_filename)\n",
    "    trunc_extracted_data = extracted_data[:15]\n",
    "    \n",
    "    model_responses = []\n",
    "    \n",
    "    for data in trunc_extracted_data:\n",
    "        context = f\"User 1: {data['user1_persona']}\\n\\n User 2: {data['user2_persona']}\\n \\\n",
    "            First 6 pieces of conversation: {data['context']}\"\n",
    "        prompt = f\"{data['prompt']}\"\n",
    "        print(\"\\n\")\n",
    "        print(f\"Context: {context}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        generated_response = generate_persona_prompt_finetuned(prompt, context)\n",
    "        print(f\"Generated Response: {generated_response}\")\n",
    "        model_responses.append(generated_response)\n",
    "    \n",
    "    return model_responses\n",
    "\n",
    "csv_filename = r'Synthetic-Persona-Chat_valid.csv'\n",
    "\n",
    "model_responses = process_validation_data(csv_filename)\n",
    "\n",
    "for response in model_responses:\n",
    "    print(\"Generated Response:\", response)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
